{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Gather information about the presence of any other titans. Prepare for war by analyzing them using this machine learning model."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Paste the video path here"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "video_path = 'BH25/Testing_Data/1.mp4' # video path"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "These are the preprocessing functions as specified in the training file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "import cv2\n",
    "import numpy as np\n",
    "def preprocess_video(video_path):\n",
    "    cap = cv2.VideoCapture(video_path)\n",
    "    frames = []\n",
    "    while True:\n",
    "        ret,frame = cap.read()\n",
    "        if not ret:\n",
    "            break\n",
    "        frame = cv2.resize(frame,(64,64))\n",
    "        frames.append(frame)\n",
    "    cap.release()\n",
    "    frames = np.array(frames)\n",
    "    frames = frames/255\n",
    "    return frames"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "import cv2\n",
    "import numpy as np\n",
    "def preprocess_video_2(video_path):\n",
    "    cap = cv2.VideoCapture(video_path)\n",
    "    frames = []\n",
    "    while True:\n",
    "        ret,frame = cap.read()\n",
    "        if not ret:\n",
    "            break\n",
    "        frame = cv2.resize(frame,(64,64))\n",
    "        frames.append(frame)\n",
    "    cap.release()\n",
    "    frames = np.array(frames)\n",
    "    frames = frames/255\n",
    "    return frames[1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "import cv2 \n",
    "import numpy as np\n",
    "def preprocess_video_3(video_path):\n",
    "    cap = cv2.VideoCapture(video_path)\n",
    "    frames = []\n",
    "    while True:\n",
    "        ret, frame = cap.read()\n",
    "        if not ret:\n",
    "            break\n",
    "        frame = cv2.resize(frame, (64, 64))\n",
    "        frames.append(frame)\n",
    "    cap.release()\n",
    "    frames = np.array(frames)  # Shape: (20, 64, 64, 3)\n",
    "    return frames.flatten()  # Flatten to 1D array"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Importing models and label encoers(for decoding)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "import joblib\n",
    "from tensorflow.keras.models import load_model\n",
    "label_encoder = {}\n",
    "categorical_columns = ['element','motion','power']\n",
    "for col in categorical_columns:\n",
    "    label_encoder[col] = joblib.load(f\"label_encoder_{col}.joblib\")\n",
    "element_model = load_model('model_element.h5', compile = False)\n",
    "motion_model = joblib.load('model_categorical_motion.joblib')\n",
    "power_model = joblib.load('model_categorical_power.joblib')\n",
    "speed_model = load_model('model_speed.h5', compile = False)\n",
    "summary_x_model = load_model('model_summary_x2.h5', compile = False)\n",
    "summary_y_model = load_model('model_summary_y2.h5', compile = False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function to predict the element in the video\n",
    "def predict_element(video_path):\n",
    "    inputs = tf.expand_dims(preprocess_video_2(video_path), axis=0)\n",
    "    predict_array = element_model.predict(inputs)\n",
    "    predict_element = label_encoder['element'].inverse_transform(tf.argmax(predict_array,axis=1))\n",
    "    return predict_element[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Predict the motion in the video\n",
    "prediction_motion = motion_model.predict(tf.expand_dims(preprocess_video_3(video_path),axis=0))\n",
    "predict_motion = label_encoder['motion'].inverse_transform(prediction_motion)\n",
    "predict_motion = predict_motion[0]\n",
    "\n",
    "#Predict the power in the video\n",
    "prediction_power = power_model.predict(tf.expand_dims(preprocess_video_3(video_path),axis=0))\n",
    "predict_power = label_encoder['power'].inverse_transform(prediction_power)\n",
    "predict_power = predict_power[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 122ms/step\n"
     ]
    }
   ],
   "source": [
    "#Predicting the speed in the video\n",
    "inputs = tf.expand_dims(preprocess_video(video_path),axis=0)\n",
    "predict_speed=speed_model.predict(inputs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 140ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 132ms/step\n"
     ]
    }
   ],
   "source": [
    "# For summary prediction\n",
    "inputs = tf.expand_dims(preprocess_video(video_path),axis=0)\n",
    "predict_summary_x=summary_x_model.predict(inputs)\n",
    "predict_summary_y=summary_y_model.predict(inputs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Converting the continuous values to numpy arrays and flattening them(converting them to 1D array)\n",
    "predict_summary_x = np.array(predict_summary_x)\n",
    "predict_summary_y = np.array(predict_summary_y)\n",
    "predict_speed = np.array(predict_speed)\n",
    "predict_summary_x = predict_summary_x.flatten()\n",
    "predict_summary_y = predict_summary_y.flatten()\n",
    "predict_speed = predict_speed.flatten()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 124ms/step\n"
     ]
    }
   ],
   "source": [
    "# Making the pandas dataframe of the predictions to show it nicely\n",
    "import pandas as pd\n",
    "predictions =pd.DataFrame({\n",
    "    'video_id':video_path,\n",
    "    'element': predict_element(video_path),  \n",
    "    'motion': predict_motion,\n",
    "    'power': predict_power,\n",
    "    'speed': predict_speed,               \n",
    "    'video_summary': list(zip(predict_summary_x, predict_summary_y)) \n",
    "})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>video_id</th>\n",
       "      <th>element</th>\n",
       "      <th>motion</th>\n",
       "      <th>power</th>\n",
       "      <th>speed</th>\n",
       "      <th>video_summary</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>BH25/Testing_Data/1.mp4</td>\n",
       "      <td>Boden</td>\n",
       "      <td>zigzag</td>\n",
       "      <td>rot</td>\n",
       "      <td>7.981758</td>\n",
       "      <td>(0.59568244, -0.16819939)</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                  video_id element  motion power     speed  \\\n",
       "0  BH25/Testing_Data/1.mp4   Boden  zigzag   rot  7.981758   \n",
       "\n",
       "               video_summary  \n",
       "0  (0.59568244, -0.16819939)  "
      ]
     },
     "execution_count": 49,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "predictions"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
